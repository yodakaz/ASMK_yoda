{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/yodakaz/ASMK_yoda/blob/main/how/Github%E3%81%8B%E3%82%89%E5%AE%9F%E8%A1%8C5.1.ipynb",
      "authorship_tag": "ABX9TyM8XrY0/pStFCH0kY9kmhOl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yodakaz/ASMK_yoda/blob/main/how/Github%E3%81%8B%E3%82%89%E5%AE%9F%E8%A1%8C5.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elBur5KeY4DZ"
      },
      "source": [
        "# clone 失敗等によるフォルダ削除用\n",
        "import shutil\n",
        "\n",
        "directory = '/content/how/how_experiments'\n",
        "\n",
        "try:\n",
        "    shutil.rmtree(directory)\n",
        "except FileNotFoundError:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCt_R1CzqGIS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2963b19f-1acb-48b2-e14b-0deaf84679e0"
      },
      "source": [
        "# 修正済みプログラムのclone\n",
        "import os\n",
        "os.chdir('/content')\n",
        "!git clone https://github.com/yodakaz/ASMK_yoda.git\n",
        "\n",
        "# howをcontent直下へ移動\n",
        "# google driveのマウント"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ASMK_yoda'...\n",
            "remote: Enumerating objects: 247, done.\u001b[K\n",
            "remote: Counting objects: 100% (247/247), done.\u001b[K\n",
            "remote: Compressing objects: 100% (232/232), done.\u001b[K\n",
            "remote: Total 247 (delta 45), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (247/247), 967.30 KiB | 3.58 MiB/s, done.\n",
            "Resolving deltas: 100% (45/45), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWmTGKQGqcAv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "9e2bcde9-ecfa-4869-fd3b-0ac261083664"
      },
      "source": [
        "# temp.pthファイルをgoogle driveからコピー\n",
        "import shutil\n",
        "shutil.copytree(\"/content/drive/MyDrive/ASMK/how_temp\", \"/content/how/how_temp\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/how/how_temp'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gif5aJAIkT7O",
        "outputId": "18ae8761-768b-4302-f283-fba943a107f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# temp.pthファイルをgoogle driveからコピー\n",
        "import shutil\n",
        "shutil.copyfile(\"/content/how/how_temp/temp.pth\", \"/content/drive/MyDrive/ASMK/how_temp/temp(20210719).pth\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/ASMK/how_temp/temp(20210719).pth'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXhtbyezYnZ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "585be2f3-de44-42f8-ba91-ddba981ad5e2"
      },
      "source": [
        "# 元コードおよびデータ clone\n",
        "import os\n",
        "os.chdir('/content')\n",
        "#!git clone https://github.com/gtolias/how.git\n",
        "\n",
        "!git clone https://github.com/MING410/local #データセットのclone"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'local'...\n",
            "remote: Enumerating objects: 5839, done.\u001b[K\n",
            "remote: Counting objects: 100% (1934/1934), done.\u001b[K\n",
            "remote: Compressing objects: 100% (135/135), done.\u001b[K\n",
            "remote: Total 5839 (delta 1799), reused 1934 (delta 1799), pack-reused 3905\u001b[K\n",
            "Receiving objects: 100% (5839/5839), 468.04 MiB | 34.50 MiB/s, done.\n",
            "Resolving deltas: 100% (3689/3689), done.\n",
            "Checking out files: 100% (7539/7539), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7Sf-klxKWtR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "090b237b-3ac5-4410-a652-af770f51e82a"
      },
      "source": [
        "# cirtorch\n",
        "import os\n",
        "os.chdir('/content')\n",
        "\n",
        "!wget \"https://github.com/filipradenovic/cnnimageretrieval-pytorch/archive/v1.2.zip\"\n",
        "!unzip v1.2.zip\n",
        "!rm v1.2.zip\n",
        "!export PYTHONPATH=${PYTHONPATH}:$(realpath cnnimageretrieval-pytorch-1.2)\n",
        "\n",
        "#\"cirtorh\"をhowに入れる"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-11 23:52:03--  https://github.com/filipradenovic/cnnimageretrieval-pytorch/archive/v1.2.zip\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/filipradenovic/cnnimageretrieval-pytorch/zip/v1.2 [following]\n",
            "--2021-07-11 23:52:03--  https://codeload.github.com/filipradenovic/cnnimageretrieval-pytorch/zip/v1.2\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.112.9\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.112.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘v1.2.zip’\n",
            "\n",
            "v1.2.zip                [ <=>                ]  41.06K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2021-07-11 23:52:03 (6.77 MB/s) - ‘v1.2.zip’ saved [42048]\n",
            "\n",
            "Archive:  v1.2.zip\n",
            "7da5c2a8c7e6d27ac652d1de137d17df31cb510a\n",
            "   creating: cnnimageretrieval-pytorch-1.2/\n",
            "  inflating: cnnimageretrieval-pytorch-1.2/LICENSE  \n",
            "  inflating: cnnimageretrieval-pytorch-1.2/README.md  \n",
            "   creating: cnnimageretrieval-pytorch-1.2/cirtorch/\n",
            "  inflating: cnnimageretrieval-pytorch-1.2/cirtorch/__init__.py  \n",
            "   creating: cnnimageretrieval-pytorch-1.2/cirtorch/datasets/\n",
            " extracting: cnnimageretrieval-pytorch-1.2/cirtorch/datasets/__init__.py  \n",
            "  inflating: cnnimageretrieval-pytorch-1.2/cirtorch/datasets/datahelpers.py  \n",
            "  inflating: cnnimageretrieval-pytorch-1.2/cirtorch/datasets/genericdataset.py  \n",
            "  inflating: cnnimageretrieval-pytorch-1.2/cirtorch/datasets/testdataset.py  \n",
            "  inflating: cnnimageretrieval-pytorch-1.2/cirtorch/datasets/traindataset.py  \n",
            "   creating: cnnimageretrieval-pytorch-1.2/cirtorch/examples/\n",
            " extracting: cnnimageretrieval-pytorch-1.2/cirtorch/examples/__init__.py  \n",
            "  inflating: cnnimageretrieval-pytorch-1.2/cirtorch/examples/example_descriptor_extraction.py  \n",
            "  inflating: cnnimageretrieval-pytorch-1.2/cirtorch/examples/test.py  \n",
            "  inflating: cnnimageretrieval-pytorch-1.2/cirtorch/examples/test_e2e.py  \n",
            "  inflating: cnnimageretrieval-pytorch-1.2/cirtorch/examples/train.py  \n",
            "   creating: cnnimageretrieval-pytorch-1.2/cirtorch/layers/\n",
            " extracting: cnnimageretrieval-pytorch-1.2/cirtorch/layers/__init__.py  \n",
            "  inflating: cnnimageretrieval-pytorch-1.2/cirtorch/layers/functional.py  \n",
            "  inflating: cnnimageretrieval-pytorch-1.2/cirtorch/layers/loss.py  \n",
            "  inflating: cnnimageretrieval-pytorch-1.2/cirtorch/layers/normalization.py  \n",
            "  inflating: cnnimageretrieval-pytorch-1.2/cirtorch/layers/pooling.py  \n",
            "   creating: cnnimageretrieval-pytorch-1.2/cirtorch/networks/\n",
            " extracting: cnnimageretrieval-pytorch-1.2/cirtorch/networks/__init__.py  \n",
            "  inflating: cnnimageretrieval-pytorch-1.2/cirtorch/networks/imageretrievalnet.py  \n",
            "   creating: cnnimageretrieval-pytorch-1.2/cirtorch/utils/\n",
            " extracting: cnnimageretrieval-pytorch-1.2/cirtorch/utils/__init__.py  \n",
            "  inflating: cnnimageretrieval-pytorch-1.2/cirtorch/utils/download.py  \n",
            "  inflating: cnnimageretrieval-pytorch-1.2/cirtorch/utils/download_win.py  \n",
            "  inflating: cnnimageretrieval-pytorch-1.2/cirtorch/utils/evaluate.py  \n",
            "  inflating: cnnimageretrieval-pytorch-1.2/cirtorch/utils/general.py  \n",
            "  inflating: cnnimageretrieval-pytorch-1.2/cirtorch/utils/whiten.py  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpVfZ0Y_Ke2Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70c5b471-629f-494a-9488-21cb96dd3832"
      },
      "source": [
        "# asmk\n",
        "import os\n",
        "os.chdir('/content')\n",
        "\n",
        "#!git clone https://github.com/jenicek/asmk.git\n",
        "!pip3 install pyaml numpy faiss-gpu\n",
        "#!cd asmk\n",
        "#!python3 /content/asmk/setup.py build_ext --inplace\n",
        "#!rm -r build\n",
        "#!cd ..\n",
        "#!export PYTHONPATH=${PYTHONPATH}:$(realpath asmk)\n",
        "#\"asmk\"(全体)をhowにいれる"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyaml\n",
            "  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Collecting faiss-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/20/cce8f99dde167453ea108f35cd4bfffcc318a314aaf1bdfb167f6be2c989/faiss_gpu-1.7.1.post2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89.7MB)\n",
            "\u001b[K     |████████████████████████████████| 89.7MB 39kB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml) (3.13)\n",
            "Installing collected packages: pyaml, faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.1.post2 pyaml-20.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEvnMVqAKni5"
      },
      "source": [
        "#不足があれば \n",
        "#(pytorchの要求バージョンが古い)\n",
        "import os\n",
        "os.chdir('/content/how')\n",
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7L5OAQ2nBgr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb002806-2807-4506-bfa2-d754c11d895b"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/how/asmk/cython')\n",
        "#/content/how/asmk/cython/setup.py を作成\n",
        "#setup.py (cython実行用)の中身\n",
        "\"\"\"\n",
        "from distutils.core import setup, Extension \n",
        "from Cython.Build import cythonize \n",
        "from numpy import get_include # cimport numpy を使うため\n",
        "\n",
        "ext = Extension(\"hamming\", sources=[\"hamming.pyx\"], include_dirs=['.', get_include()])\n",
        "setup(name=\"hamming\", ext_modules=cythonize([ext]))\n",
        "\"\"\"\n",
        "#実行\n",
        "!python setup.py build_ext --inplace"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running build_ext\n",
            "building 'hamming' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.7\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-OGiuun/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-OGiuun/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I. -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I/usr/include/python3.7m -c hamming.c -o build/temp.linux-x86_64-3.7/hamming.o\n",
            "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-OGiuun/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/hamming.o -o /content/how/asmk/cython/hamming.cpython-37m-x86_64-linux-gnu.so\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlII0dUnTSHF",
        "outputId": "51edfb1f-85c5-49b4-97f2-78e3bfa468c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# ローカルへ保存用\n",
        "!zip -r /content/exp_0719.zip /content/how/how_data/experiments"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/how/how_data/experiments/ (stored 0%)\n",
            "  adding: content/how/how_data/experiments/train_how_r18/ (stored 0%)\n",
            "  adding: content/how/how_data/experiments/train_how_r18/query_results.pkl (deflated 60%)\n",
            "  adding: content/how/how_data/experiments/train_how_r18/train_params.yml (deflated 58%)\n",
            "  adding: content/how/how_data/experiments/train_how_r18/fig_train.jpg (deflated 25%)\n",
            "  adding: content/how/how_data/experiments/train_how_r18/train.log (deflated 73%)\n",
            "  adding: content/how/how_data/experiments/train_how_r18/epochs/ (stored 0%)\n",
            "  adding: content/how/how_data/experiments/train_how_r18/epochs/model_best.pth (deflated 7%)\n",
            "  adding: content/how/how_data/experiments/train_how_r18/fig_val_local_descriptor.jpg (deflated 51%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4PWxCO1MmSx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f72c2eb-8c73-4435-93f1-2d5e2638bbc4"
      },
      "source": [
        "#コードの実行\n",
        "import os\n",
        "os.chdir('/content/how')\n",
        "\n",
        "########### 元コード変更箇所 ##################################################\n",
        "#./stage/evaluate.py:asmk.asmk_method→asmk.asmk.asmk_method\n",
        "#./asmk/kernel.py:import . →import ..cython from hamming\n",
        "#./examples/demo_how.py:(L90)コメントアウト(自前データセット使用のため)\n",
        "#./how/utils/data_helper.py:(L14～)training_set = 'mitsubishi_dataset'\n",
        "#                           ims_root:'ims' → 'png_images'\n",
        "#                           db = pickle.load(f)#['train'] ← コメントアウト\n",
        "#       同  elif dataset == 'val_eccv20':\n",
        "#              db_root,  fn_val_proper, ims_root を変更\n",
        "#              ※fn_val_proper = db_root+'/mitsubishi_dataset_val-eccv2020.pkl'\n",
        "#./cirtorch/datasets/datahelper.py:return os.path.join()変更\n",
        "#./cirtorch/datasets/traindataset.py:(L50～).ymlから読み込んだ情報修正\n",
        "#if name == 'retrieval-SfM-120k':\n",
        "#  print(\"changed retrieval-SfM-120k => mitsubishi_dataset\")\n",
        "#  name = 'mitsubishi_dataset'\n",
        "#ims_rootの'ims' → 'png_images'\n",
        "#db = pickle.load(f)#[mode] ←コメントアウト\n",
        "#/conent/how/how_data/train/mitsubishi_dataset作成\n",
        "#\".pkl\"ファイル2つとlocalからpng_images移動\n",
        "# /how/how_tempを作成\n",
        "\n",
        "#!python3 examples/demo_how.py eval examples/params/eccv20/eval_how_r18_1000.yml -e official_how_r18_1000\n",
        "!python3 examples/demo_how.py train examples/params/eccv20/train_how_r18.yml -e train_how_r18"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HOW INFO: Initializing network whitening\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            ">>>> 989/989 done...\n",
            "tcmalloc: large alloc 1490747392 bytes == 0x55dbd8eb2000 @  0x7fc0fe9711e7 0x7fc0a1d7446e 0x7fc0a1dc4c7b 0x7fc0a1dc4d18 0x7fc0a1e6c010 0x7fc0a1e6c73c 0x7fc0a1e6c85d 0x55dae6f612b8 0x7fc0a1db1ef7 0x55dae6f5ef97 0x55dae6f5eda0 0x55dae6fd2bb3 0x55dae6fcdc35 0x55dae6f6073a 0x55dae6fd2f40 0x55dae6e9fd14 0x7fc0a1db1ef7 0x55dae6f5ef97 0x55dae6f5eda0 0x55dae6fd2bb3 0x55dae6fcdc35 0x55dae6f6073a 0x55dae6fd2f40 0x55dae6fce235 0x55dae6f6073a 0x55dae6fcf93b 0x55dae6fce235 0x55dae6e9fe2c 0x55dae6fd0318 0x55dae6fcdc35 0x55dae6e9fe2c\n",
            "tcmalloc: large alloc 1490747392 bytes == 0x55dc3246e000 @  0x7fc0fe9711e7 0x7fc0a1d7446e 0x7fc0a1dc4c7b 0x7fc0a1dc4d18 0x7fc0a1e80d79 0x7fc0a1e83e4c 0x7fc0a1fa2e7f 0x7fc0a1fa8fb5 0x7fc0a1faae3d 0x7fc0a1fac516 0x55dae6f60280 0x55dae6f5fe59 0x7fc0a1e8b0db 0x55dae7048e52 0x55dae6fcf834 0x55dae6fcdc35 0x55dae6f6073a 0x55dae6fd2f40 0x55dae6f6065a 0x55dae6fd2f40 0x55dae6fce235 0x55dae6e9fe2c 0x55dae6fd0318 0x55dae6fcdc35 0x55dae6e9fe2c 0x55dae6fd0318 0x55dae6f6065a 0x55dae6fceb0e 0x55dae6fcdc35 0x55dae6fcd933 0x55dae7097402\n",
            "changed retrieval-SfM-120k => mitsubishi_dataset\n",
            ">> Creating tuples for an epoch of mitsubishi_dataset-train...\n",
            ">>>> used network: \n",
            "HOWNet(meta={\n",
            "    architecture: resnet18\n",
            "    backbone_dim: 512\n",
            "    outputdim: 128\n",
            "    corercf_size: 32\n",
            "})\n",
            ">> Extracting descriptors for query images...\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            ">>>> 644/644 done...\n",
            ">> Extracting descriptors for negative pool...\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            ">>>> 989/989 done...\n",
            ">> Searching for hard negatives...\n",
            ">>>> Average negative l2-distance: 1.01\n",
            ">>>> Done\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:579: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(other, self)\n",
            "HOW INFO: >> Train: [1][1/128]\tTime 24.337 (24.337)\tData 21.856 (21.856)\tLoss 1.0773 (0.9265)\n",
            "HOW INFO: >> Train: [1][20/128]\tTime 1.824 (3.513)\tData 0.000 (1.610)\tLoss 0.4332 (0.5467)\n",
            "HOW INFO: >> Train: [1][40/128]\tTime 1.781 (2.990)\tData 0.005 (1.108)\tLoss 0.4853 (0.5018)\n",
            "HOW INFO: >> Train: [1][60/128]\tTime 2.215 (2.856)\tData 0.003 (0.988)\tLoss 0.4136 (0.4797)\n",
            "HOW INFO: >> Train: [1][80/128]\tTime 1.789 (2.808)\tData 0.008 (0.954)\tLoss 0.4748 (0.4644)\n",
            "HOW INFO: >> Train: [1][100/128]\tTime 1.774 (2.744)\tData 0.010 (0.900)\tLoss 0.4462 (0.4503)\n",
            "HOW INFO: >> Train: [1][120/128]\tTime 1.731 (2.687)\tData 0.000 (0.850)\tLoss 0.4079 (0.4403)\n",
            "HOW INFO: >> Train: [1][128/128]\tTime 1.577 (2.645)\tData 0.000 (0.823)\tLoss 0.4211 (0.4370)\n",
            "!! show frequency !! None\n",
            "!! change frequency==22 !!\n",
            "!! show frequency !! 5\n",
            "!! change frequency==22 !!\n",
            "#######check saved content######\n",
            "scores :  {'global_descriptor': defaultdict(<class 'list'>, {}), 'local_descriptor': defaultdict(<class 'list'>, {}), 'train_loss': [(1, 0.4370477388612926)]}\n",
            "net_params :  {'architecture': 'resnet18', 'pretrained': True, 'skip_layer': 0, 'dim_reduction': {'dim': 128}, 'smoothing': {'kernel_size': 3}, 'runtime': {'mean_std': [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]], 'image_size': 1024, 'features_num': 1000, 'scales': [2.0, 1.414, 1.0, 0.707, 0.5, 0.353, 0.25], 'training_scales': [1]}}\n",
            "losslogger :  0.4370477388612926\n",
            "scheduler :  <torch.optim.lr_scheduler.ExponentialLR object at 0x7fc07d60a690>\n",
            "######saved after 1#######\n",
            "HOW INFO: Epoch 1 finished in 453.5s\n",
            "#######check loaded content######\n",
            "scores :  {'global_descriptor': defaultdict(<class 'list'>, {}), 'local_descriptor': defaultdict(<class 'list'>, {}), 'train_loss': [(1, 0.4370477388612926)]}\n",
            "net_params :  {'architecture': 'resnet18', 'pretrained': True, 'skip_layer': 0, 'dim_reduction': {'dim': 128}, 'smoothing': {'kernel_size': 3}, 'runtime': {'mean_std': [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]], 'image_size': 1024, 'features_num': 1000, 'scales': [2.0, 1.414, 1.0, 0.707, 0.5, 0.353, 0.25], 'training_scales': [1]}}\n",
            "losslogger :  0.4370477388612926\n",
            "######loaded at 2#######\n",
            ">> Creating tuples for an epoch of mitsubishi_dataset-train...\n",
            ">>>> used network: \n",
            "HOWNet(meta={\n",
            "    architecture: resnet18\n",
            "    backbone_dim: 512\n",
            "    outputdim: 128\n",
            "    corercf_size: 32\n",
            "})\n",
            ">> Extracting descriptors for query images...\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            ">>>> 644/644 done...\n",
            ">> Extracting descriptors for negative pool...\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            ">>>> 989/989 done...\n",
            ">> Searching for hard negatives...\n",
            ">>>> Average negative l2-distance: 0.55\n",
            ">>>> Done\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "HOW INFO: >> Train: [2][1/128]\tTime 19.853 (19.853)\tData 17.358 (17.358)\tLoss 0.4126 (0.4245)\n",
            "HOW INFO: >> Train: [2][20/128]\tTime 1.817 (3.474)\tData 0.004 (1.615)\tLoss 0.3651 (0.4049)\n",
            "HOW INFO: >> Train: [2][40/128]\tTime 1.789 (2.988)\tData 0.000 (1.160)\tLoss 0.4040 (0.3939)\n",
            "HOW INFO: >> Train: [2][60/128]\tTime 1.803 (2.804)\tData 0.000 (0.978)\tLoss 0.3703 (0.3889)\n",
            "HOW INFO: >> Train: [2][80/128]\tTime 1.782 (2.776)\tData 0.000 (0.940)\tLoss 0.4605 (0.3887)\n",
            "HOW INFO: >> Train: [2][100/128]\tTime 1.813 (2.718)\tData 0.009 (0.881)\tLoss 0.4636 (0.3855)\n",
            "HOW INFO: >> Train: [2][120/128]\tTime 1.785 (2.678)\tData 0.000 (0.836)\tLoss 0.3610 (0.3825)\n",
            "HOW INFO: >> Train: [2][128/128]\tTime 1.587 (2.637)\tData 0.000 (0.808)\tLoss 0.4412 (0.3812)\n",
            "!! show frequency !! None\n",
            "!! change frequency==22 !!\n",
            "!! show frequency !! 5\n",
            "!! change frequency==22 !!\n",
            "#######check saved content######\n",
            "scores :  {'global_descriptor': defaultdict(<class 'list'>, {}), 'local_descriptor': defaultdict(<class 'list'>, {}), 'train_loss': [(1, 0.4370477388612926), (2, 0.3812167121330276)]}\n",
            "net_params :  {'architecture': 'resnet18', 'pretrained': True, 'skip_layer': 0, 'dim_reduction': {'dim': 128}, 'smoothing': {'kernel_size': 3}, 'runtime': {'mean_std': [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]], 'image_size': 1024, 'features_num': 1000, 'scales': [2.0, 1.414, 1.0, 0.707, 0.5, 0.353, 0.25], 'training_scales': [1]}}\n",
            "losslogger :  0.3812167121330276\n",
            "scheduler :  <torch.optim.lr_scheduler.ExponentialLR object at 0x7fc0744fe190>\n",
            "######saved after 2#######\n",
            "HOW INFO: Epoch 2 finished in 452.9s\n",
            "#######check loaded content######\n",
            "scores :  {'global_descriptor': defaultdict(<class 'list'>, {}), 'local_descriptor': defaultdict(<class 'list'>, {}), 'train_loss': [(1, 0.4370477388612926), (2, 0.3812167121330276)]}\n",
            "net_params :  {'architecture': 'resnet18', 'pretrained': True, 'skip_layer': 0, 'dim_reduction': {'dim': 128}, 'smoothing': {'kernel_size': 3}, 'runtime': {'mean_std': [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]], 'image_size': 1024, 'features_num': 1000, 'scales': [2.0, 1.414, 1.0, 0.707, 0.5, 0.353, 0.25], 'training_scales': [1]}}\n",
            "losslogger :  0.3812167121330276\n",
            "######loaded at 3#######\n",
            ">> Creating tuples for an epoch of mitsubishi_dataset-train...\n",
            ">>>> used network: \n",
            "HOWNet(meta={\n",
            "    architecture: resnet18\n",
            "    backbone_dim: 512\n",
            "    outputdim: 128\n",
            "    corercf_size: 32\n",
            "})\n",
            ">> Extracting descriptors for query images...\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            ">>>> 644/644 done...\n",
            ">> Extracting descriptors for negative pool...\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            ">>>> 989/989 done...\n",
            ">> Searching for hard negatives...\n",
            ">>>> Average negative l2-distance: 0.59\n",
            ">>>> Done\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "HOW INFO: >> Train: [3][1/128]\tTime 19.601 (19.601)\tData 16.817 (16.817)\tLoss 0.3638 (0.4094)\n",
            "HOW INFO: >> Train: [3][20/128]\tTime 1.821 (3.397)\tData 0.004 (1.479)\tLoss 0.3676 (0.3704)\n",
            "HOW INFO: >> Train: [3][40/128]\tTime 1.811 (2.955)\tData 0.010 (1.100)\tLoss 0.4295 (0.3613)\n",
            "HOW INFO: >> Train: [3][60/128]\tTime 1.847 (2.793)\tData 0.000 (0.955)\tLoss 0.4093 (0.3591)\n",
            "HOW INFO: >> Train: [3][80/128]\tTime 1.754 (2.765)\tData 0.005 (0.934)\tLoss 0.3239 (0.3552)\n",
            "HOW INFO: >> Train: [3][100/128]\tTime 1.775 (2.712)\tData 0.012 (0.887)\tLoss 0.2255 (0.3526)\n",
            "HOW INFO: >> Train: [3][120/128]\tTime 1.761 (2.669)\tData 0.000 (0.849)\tLoss 0.3951 (0.3511)\n",
            "HOW INFO: >> Train: [3][128/128]\tTime 1.583 (2.623)\tData 0.000 (0.815)\tLoss 0.2352 (0.3501)\n",
            "!! show frequency !! None\n",
            "!! change frequency==22 !!\n",
            "!! show frequency !! 5\n",
            "!! change frequency==22 !!\n",
            "#######check saved content######\n",
            "scores :  {'global_descriptor': defaultdict(<class 'list'>, {}), 'local_descriptor': defaultdict(<class 'list'>, {}), 'train_loss': [(1, 0.4370477388612926), (2, 0.3812167121330276), (3, 0.3500754334963858)]}\n",
            "net_params :  {'architecture': 'resnet18', 'pretrained': True, 'skip_layer': 0, 'dim_reduction': {'dim': 128}, 'smoothing': {'kernel_size': 3}, 'runtime': {'mean_std': [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]], 'image_size': 1024, 'features_num': 1000, 'scales': [2.0, 1.414, 1.0, 0.707, 0.5, 0.353, 0.25], 'training_scales': [1]}}\n",
            "losslogger :  0.3500754334963858\n",
            "scheduler :  <torch.optim.lr_scheduler.ExponentialLR object at 0x7fc074527c10>\n",
            "######saved after 3#######\n",
            "HOW INFO: Epoch 3 finished in 450.4s\n",
            "#######check loaded content######\n",
            "scores :  {'global_descriptor': defaultdict(<class 'list'>, {}), 'local_descriptor': defaultdict(<class 'list'>, {}), 'train_loss': [(1, 0.4370477388612926), (2, 0.3812167121330276), (3, 0.3500754334963858)]}\n",
            "net_params :  {'architecture': 'resnet18', 'pretrained': True, 'skip_layer': 0, 'dim_reduction': {'dim': 128}, 'smoothing': {'kernel_size': 3}, 'runtime': {'mean_std': [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]], 'image_size': 1024, 'features_num': 1000, 'scales': [2.0, 1.414, 1.0, 0.707, 0.5, 0.353, 0.25], 'training_scales': [1]}}\n",
            "losslogger :  0.3500754334963858\n",
            "######loaded at 4#######\n",
            ">> Creating tuples for an epoch of mitsubishi_dataset-train...\n",
            ">>>> used network: \n",
            "HOWNet(meta={\n",
            "    architecture: resnet18\n",
            "    backbone_dim: 512\n",
            "    outputdim: 128\n",
            "    corercf_size: 32\n",
            "})\n",
            ">> Extracting descriptors for query images...\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            ">>>> 644/644 done...\n",
            ">> Extracting descriptors for negative pool...\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            ">>>> 989/989 done...\n",
            ">> Searching for hard negatives...\n",
            ">>>> Average negative l2-distance: 0.59\n",
            ">>>> Done\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "HOW INFO: >> Train: [4][1/128]\tTime 17.136 (17.136)\tData 14.311 (14.311)\tLoss 0.3122 (0.3408)\n",
            "HOW INFO: >> Train: [4][20/128]\tTime 1.978 (3.248)\tData 0.000 (1.235)\tLoss 0.3609 (0.3567)\n",
            "HOW INFO: >> Train: [4][40/128]\tTime 1.757 (2.870)\tData 0.000 (0.901)\tLoss 0.3908 (0.3462)\n",
            "HOW INFO: >> Train: [4][60/128]\tTime 1.759 (2.738)\tData 0.004 (0.791)\tLoss 0.2978 (0.3398)\n",
            "HOW INFO: >> Train: [4][80/128]\tTime 2.011 (2.714)\tData 0.000 (0.764)\tLoss 0.2696 (0.3357)\n",
            "HOW INFO: >> Train: [4][100/128]\tTime 1.740 (2.657)\tData 0.000 (0.711)\tLoss 0.2614 (0.3313)\n",
            "HOW INFO: >> Train: [4][120/128]\tTime 1.797 (2.620)\tData 0.000 (0.669)\tLoss 0.3155 (0.3291)\n",
            "HOW INFO: >> Train: [4][128/128]\tTime 1.582 (2.585)\tData 0.000 (0.652)\tLoss 0.3138 (0.3283)\n",
            "!! show frequency !! None\n",
            "!! change frequency==22 !!\n",
            "!! show frequency !! 5\n",
            "!! change frequency==22 !!\n",
            "#######check saved content######\n",
            "scores :  {'global_descriptor': defaultdict(<class 'list'>, {}), 'local_descriptor': defaultdict(<class 'list'>, {}), 'train_loss': [(1, 0.4370477388612926), (2, 0.3812167121330276), (3, 0.3500754334963858), (4, 0.3283409605035558)]}\n",
            "net_params :  {'architecture': 'resnet18', 'pretrained': True, 'skip_layer': 0, 'dim_reduction': {'dim': 128}, 'smoothing': {'kernel_size': 3}, 'runtime': {'mean_std': [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]], 'image_size': 1024, 'features_num': 1000, 'scales': [2.0, 1.414, 1.0, 0.707, 0.5, 0.353, 0.25], 'training_scales': [1]}}\n",
            "losslogger :  0.3283409605035558\n",
            "scheduler :  <torch.optim.lr_scheduler.ExponentialLR object at 0x7fc0745482d0>\n",
            "######saved after 4#######\n",
            "HOW INFO: Epoch 4 finished in 446.2s\n",
            "#######check loaded content######\n",
            "scores :  {'global_descriptor': defaultdict(<class 'list'>, {}), 'local_descriptor': defaultdict(<class 'list'>, {}), 'train_loss': [(1, 0.4370477388612926), (2, 0.3812167121330276), (3, 0.3500754334963858), (4, 0.3283409605035558)]}\n",
            "net_params :  {'architecture': 'resnet18', 'pretrained': True, 'skip_layer': 0, 'dim_reduction': {'dim': 128}, 'smoothing': {'kernel_size': 3}, 'runtime': {'mean_std': [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]], 'image_size': 1024, 'features_num': 1000, 'scales': [2.0, 1.414, 1.0, 0.707, 0.5, 0.353, 0.25], 'training_scales': [1]}}\n",
            "losslogger :  0.3283409605035558\n",
            "######loaded at 5#######\n",
            ">> Creating tuples for an epoch of mitsubishi_dataset-train...\n",
            ">>>> used network: \n",
            "HOWNet(meta={\n",
            "    architecture: resnet18\n",
            "    backbone_dim: 512\n",
            "    outputdim: 128\n",
            "    corercf_size: 32\n",
            "})\n",
            ">> Extracting descriptors for query images...\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            ">>>> 644/644 done...\n",
            ">> Extracting descriptors for negative pool...\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            ">>>> 989/989 done...\n",
            ">> Searching for hard negatives...\n",
            ">>>> Average negative l2-distance: 0.56\n",
            ">>>> Done\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "HOW INFO: >> Train: [5][1/128]\tTime 19.143 (19.143)\tData 16.352 (16.352)\tLoss 0.3504 (0.3978)\n",
            "HOW INFO: >> Train: [5][20/128]\tTime 2.997 (3.251)\tData 1.229 (1.320)\tLoss 0.2437 (0.3483)\n",
            "HOW INFO: >> Train: [5][40/128]\tTime 1.794 (2.848)\tData 0.000 (0.925)\tLoss 0.3580 (0.3347)\n",
            "HOW INFO: >> Train: [5][60/128]\tTime 1.759 (2.721)\tData 0.000 (0.805)\tLoss 0.3234 (0.3259)\n",
            "HOW INFO: >> Train: [5][80/128]\tTime 2.001 (2.708)\tData 0.000 (0.765)\tLoss 0.2244 (0.3210)\n",
            "HOW INFO: >> Train: [5][100/128]\tTime 1.798 (2.643)\tData 0.005 (0.703)\tLoss 0.3567 (0.3175)\n",
            "HOW INFO: >> Train: [5][120/128]\tTime 1.807 (2.625)\tData 0.000 (0.684)\tLoss 0.3429 (0.3141)\n",
            "HOW INFO: >> Train: [5][128/128]\tTime 1.586 (2.587)\tData 0.000 (0.666)\tLoss 0.2450 (0.3130)\n",
            "!! show frequency !! None\n",
            "!! change frequency==22 !!\n",
            "!! show frequency !! 5\n",
            "!! change frequency==22 !!\n",
            "#######check saved content######\n",
            "scores :  {'global_descriptor': defaultdict(<class 'list'>, {}), 'local_descriptor': defaultdict(<class 'list'>, {}), 'train_loss': [(1, 0.4370477388612926), (2, 0.3812167121330276), (3, 0.3500754334963858), (4, 0.3283409605035558), (5, 0.3130098097026348)]}\n",
            "net_params :  {'architecture': 'resnet18', 'pretrained': True, 'skip_layer': 0, 'dim_reduction': {'dim': 128}, 'smoothing': {'kernel_size': 3}, 'runtime': {'mean_std': [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]], 'image_size': 1024, 'features_num': 1000, 'scales': [2.0, 1.414, 1.0, 0.707, 0.5, 0.353, 0.25], 'training_scales': [1]}}\n",
            "losslogger :  0.3130098097026348\n",
            "scheduler :  <torch.optim.lr_scheduler.ExponentialLR object at 0x7fc0746e8d90>\n",
            "######saved after 5#######\n",
            "HOW INFO: Epoch 5 finished in 446.8s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}